{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "net = ResNet(BasicBlock, [2, 2, 2, 2], 10) #ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   250] loss: 0.3008520\n",
      "[2,   250] loss: 0.2513619\n",
      "[3,   250] loss: 0.2079939\n",
      "[4,   250] loss: 0.1800726\n",
      "[5,   250] loss: 0.1600693\n",
      "[6,   250] loss: 0.1401998\n",
      "[7,   250] loss: 0.1292284\n",
      "[8,   250] loss: 0.1154520\n",
      "[9,   250] loss: 0.1072027\n",
      "[10,   250] loss: 0.0991769\n",
      "[11,   250] loss: 0.0870239\n",
      "[12,   250] loss: 0.0821714\n",
      "[13,   250] loss: 0.0744525\n",
      "[14,   250] loss: 0.0698021\n",
      "[15,   250] loss: 0.0644497\n",
      "[16,   250] loss: 0.0616760\n",
      "[17,   250] loss: 0.0553137\n",
      "[18,   250] loss: 0.0559582\n",
      "[19,   250] loss: 0.0507068\n",
      "[20,   250] loss: 0.0443818\n",
      "[21,   250] loss: 0.0439139\n",
      "[22,   250] loss: 0.0466457\n",
      "[23,   250] loss: 0.0347619\n",
      "[24,   250] loss: 0.0391996\n",
      "[25,   250] loss: 0.0345514\n",
      "[26,   250] loss: 0.0365688\n",
      "[27,   250] loss: 0.0306552\n",
      "[28,   250] loss: 0.0317909\n",
      "[29,   250] loss: 0.0303260\n",
      "[30,   250] loss: 0.0259798\n",
      "[31,   250] loss: 0.0286038\n",
      "[32,   250] loss: 0.0227758\n",
      "[33,   250] loss: 0.0215041\n",
      "[34,   250] loss: 0.0238506\n",
      "[35,   250] loss: 0.0220235\n",
      "[36,   250] loss: 0.0185377\n",
      "[37,   250] loss: 0.0215158\n",
      "[38,   250] loss: 0.0204639\n",
      "[39,   250] loss: 0.0205342\n",
      "[40,   250] loss: 0.0200394\n",
      "[41,   250] loss: 0.0179407\n",
      "[42,   250] loss: 0.0183405\n",
      "[43,   250] loss: 0.0157478\n",
      "[44,   250] loss: 0.0163444\n",
      "[45,   250] loss: 0.0195800\n",
      "[46,   250] loss: 0.0148948\n",
      "[47,   250] loss: 0.0142326\n",
      "[48,   250] loss: 0.0141466\n",
      "[49,   250] loss: 0.0182222\n",
      "[50,   250] loss: 0.0128736\n",
      "[51,   250] loss: 0.0123249\n",
      "[52,   250] loss: 0.0120775\n",
      "[53,   250] loss: 0.0117566\n",
      "[54,   250] loss: 0.0136807\n",
      "[55,   250] loss: 0.0097368\n",
      "[56,   250] loss: 0.0087716\n",
      "[57,   250] loss: 0.0114332\n",
      "[58,   250] loss: 0.0129160\n",
      "[59,   250] loss: 0.0096524\n",
      "[60,   250] loss: 0.0089619\n",
      "[61,   250] loss: 0.0130722\n",
      "[62,   250] loss: 0.0080629\n",
      "[63,   250] loss: 0.0130305\n",
      "[64,   250] loss: 0.0084753\n",
      "[65,   250] loss: 0.0071109\n",
      "[66,   250] loss: 0.0084377\n",
      "[67,   250] loss: 0.0088628\n",
      "[68,   250] loss: 0.0108851\n",
      "[69,   250] loss: 0.0073793\n",
      "[70,   250] loss: 0.0067941\n",
      "[71,   250] loss: 0.0070711\n",
      "[72,   250] loss: 0.0074536\n",
      "[73,   250] loss: 0.0064116\n",
      "[74,   250] loss: 0.0115179\n",
      "[75,   250] loss: 0.0091533\n",
      "[76,   250] loss: 0.0077964\n",
      "[77,   250] loss: 0.0055245\n",
      "[78,   250] loss: 0.0082518\n",
      "[79,   250] loss: 0.0078077\n",
      "[80,   250] loss: 0.0080917\n",
      "[81,   250] loss: 0.0046435\n",
      "[82,   250] loss: 0.0056642\n",
      "[83,   250] loss: 0.0062585\n",
      "[84,   250] loss: 0.0070711\n",
      "[85,   250] loss: 0.0057737\n",
      "[86,   250] loss: 0.0052578\n",
      "[87,   250] loss: 0.0038021\n",
      "[88,   250] loss: 0.0060063\n",
      "[89,   250] loss: 0.0053653\n",
      "[90,   250] loss: 0.0049493\n",
      "[91,   250] loss: 0.0052354\n",
      "[92,   250] loss: 0.0064850\n",
      "[93,   250] loss: 0.0048670\n",
      "[94,   250] loss: 0.0080227\n",
      "[95,   250] loss: 0.0054175\n",
      "[96,   250] loss: 0.0052996\n",
      "[97,   250] loss: 0.0038722\n",
      "[98,   250] loss: 0.0030911\n",
      "[99,   250] loss: 0.0062483\n",
      "[100,   250] loss: 0.0068540\n",
      "[101,   250] loss: 0.0053384\n",
      "[102,   250] loss: 0.0064062\n",
      "[103,   250] loss: 0.0045955\n",
      "[104,   250] loss: 0.0033385\n",
      "[105,   250] loss: 0.0058713\n",
      "[106,   250] loss: 0.0053260\n",
      "[107,   250] loss: 0.0060132\n",
      "[108,   250] loss: 0.0066827\n",
      "[109,   250] loss: 0.0054895\n",
      "[110,   250] loss: 0.0061651\n",
      "[111,   250] loss: 0.0064984\n",
      "[112,   250] loss: 0.0050835\n",
      "[113,   250] loss: 0.0035040\n",
      "[114,   250] loss: 0.0031385\n",
      "[115,   250] loss: 0.0051189\n",
      "[116,   250] loss: 0.0038100\n",
      "[117,   250] loss: 0.0026083\n",
      "[118,   250] loss: 0.0032804\n",
      "[119,   250] loss: 0.0055819\n",
      "[120,   250] loss: 0.0061029\n",
      "[121,   250] loss: 0.0034520\n",
      "[122,   250] loss: 0.0040884\n",
      "[123,   250] loss: 0.0018723\n",
      "[124,   250] loss: 0.0031344\n",
      "[125,   250] loss: 0.0031144\n",
      "[126,   250] loss: 0.0042025\n",
      "[127,   250] loss: 0.0041711\n",
      "[128,   250] loss: 0.0043859\n",
      "[129,   250] loss: 0.0045736\n",
      "[130,   250] loss: 0.0060857\n",
      "[131,   250] loss: 0.0034878\n",
      "[132,   250] loss: 0.0034782\n",
      "[133,   250] loss: 0.0042598\n",
      "[134,   250] loss: 0.0023017\n",
      "[135,   250] loss: 0.0023703\n",
      "[136,   250] loss: 0.0047209\n",
      "[137,   250] loss: 0.0027405\n",
      "[138,   250] loss: 0.0016850\n",
      "[139,   250] loss: 0.0032903\n",
      "[140,   250] loss: 0.0028394\n",
      "[141,   250] loss: 0.0026116\n",
      "[142,   250] loss: 0.0029541\n",
      "[143,   250] loss: 0.0031808\n",
      "[144,   250] loss: 0.0036162\n",
      "[145,   250] loss: 0.0025496\n",
      "[146,   250] loss: 0.0017301\n",
      "[147,   250] loss: 0.0021047\n",
      "[148,   250] loss: 0.0035352\n",
      "[149,   250] loss: 0.0038393\n",
      "[150,   250] loss: 0.0027579\n",
      "[151,   250] loss: 0.0018947\n",
      "[152,   250] loss: 0.0013115\n",
      "[153,   250] loss: 0.0008153\n",
      "[154,   250] loss: 0.0009328\n",
      "[155,   250] loss: 0.0008389\n",
      "[156,   250] loss: 0.0006037\n",
      "[157,   250] loss: 0.0004951\n",
      "[158,   250] loss: 0.0007337\n",
      "[159,   250] loss: 0.0003211\n",
      "[160,   250] loss: 0.0005427\n",
      "[161,   250] loss: 0.0004585\n",
      "[162,   250] loss: 0.0005354\n",
      "[163,   250] loss: 0.0004333\n",
      "[164,   250] loss: 0.0002222\n",
      "[165,   250] loss: 0.0003949\n",
      "[166,   250] loss: 0.0003827\n",
      "[167,   250] loss: 0.0002374\n",
      "[168,   250] loss: 0.0005724\n",
      "[169,   250] loss: 0.0004578\n",
      "[170,   250] loss: 0.0004617\n",
      "[171,   250] loss: 0.0002105\n",
      "[172,   250] loss: 0.0003040\n",
      "[173,   250] loss: 0.0003766\n",
      "[174,   250] loss: 0.0002369\n",
      "[175,   250] loss: 0.0002630\n",
      "[176,   250] loss: 0.0005147\n",
      "[177,   250] loss: 0.0002252\n",
      "[178,   250] loss: 0.0001827\n",
      "[179,   250] loss: 0.0002265\n",
      "[180,   250] loss: 0.0001919\n",
      "[181,   250] loss: 0.0001818\n",
      "[182,   250] loss: 0.0002778\n",
      "[183,   250] loss: 0.0001805\n",
      "[184,   250] loss: 0.0003635\n",
      "[185,   250] loss: 0.0001936\n",
      "[186,   250] loss: 0.0002095\n",
      "[187,   250] loss: 0.0002512\n",
      "[188,   250] loss: 0.0001251\n",
      "[189,   250] loss: 0.0002057\n",
      "[190,   250] loss: 0.0002103\n",
      "[191,   250] loss: 0.0001408\n",
      "[192,   250] loss: 0.0001393\n",
      "[193,   250] loss: 0.0001760\n",
      "[194,   250] loss: 0.0002290\n",
      "[195,   250] loss: 0.0003173\n",
      "[196,   250] loss: 0.0001140\n",
      "[197,   250] loss: 0.0001826\n",
      "[198,   250] loss: 0.0003707\n",
      "[199,   250] loss: 0.0001542\n",
      "[200,   250] loss: 0.0001825\n",
      "[201,   250] loss: 0.0001259\n",
      "[202,   250] loss: 0.0001758\n",
      "[203,   250] loss: 0.0002034\n",
      "[204,   250] loss: 0.0001161\n",
      "[205,   250] loss: 0.0002326\n",
      "[206,   250] loss: 0.0000916\n",
      "[207,   250] loss: 0.0002211\n",
      "[208,   250] loss: 0.0001999\n",
      "[209,   250] loss: 0.0001251\n",
      "[210,   250] loss: 0.0001109\n",
      "[211,   250] loss: 0.0001308\n",
      "[212,   250] loss: 0.0000926\n",
      "[213,   250] loss: 0.0001838\n",
      "[214,   250] loss: 0.0001998\n",
      "[215,   250] loss: 0.0001092\n",
      "[216,   250] loss: 0.0000917\n",
      "[217,   250] loss: 0.0001068\n",
      "[218,   250] loss: 0.0000798\n",
      "[219,   250] loss: 0.0002451\n",
      "[220,   250] loss: 0.0001749\n",
      "[221,   250] loss: 0.0001850\n",
      "[222,   250] loss: 0.0001593\n",
      "[223,   250] loss: 0.0000779\n",
      "[224,   250] loss: 0.0000943\n",
      "[225,   250] loss: 0.0002332\n",
      "[226,   250] loss: 0.0001739\n",
      "[227,   250] loss: 0.0000575\n",
      "[228,   250] loss: 0.0001222\n",
      "[229,   250] loss: 0.0000646\n",
      "[230,   250] loss: 0.0001323\n",
      "[231,   250] loss: 0.0000592\n",
      "[232,   250] loss: 0.0001442\n",
      "[233,   250] loss: 0.0001257\n",
      "[234,   250] loss: 0.0000976\n",
      "[235,   250] loss: 0.0001031\n",
      "[236,   250] loss: 0.0001285\n",
      "[237,   250] loss: 0.0000786\n",
      "[238,   250] loss: 0.0001205\n",
      "[239,   250] loss: 0.0001227\n",
      "[240,   250] loss: 0.0000969\n",
      "[241,   250] loss: 0.0001234\n",
      "[242,   250] loss: 0.0000989\n",
      "[243,   250] loss: 0.0000956\n",
      "[244,   250] loss: 0.0001611\n",
      "[245,   250] loss: 0.0000945\n",
      "[246,   250] loss: 0.0001085\n",
      "[247,   250] loss: 0.0001002\n",
      "[248,   250] loss: 0.0000806\n",
      "[249,   250] loss: 0.0001558\n",
      "[250,   250] loss: 0.0001050\n",
      "[251,   250] loss: 0.0002601\n",
      "[252,   250] loss: 0.0000633\n",
      "[253,   250] loss: 0.0000884\n",
      "[254,   250] loss: 0.0001000\n",
      "[255,   250] loss: 0.0001152\n",
      "[256,   250] loss: 0.0000632\n",
      "[257,   250] loss: 0.0000785\n",
      "[258,   250] loss: 0.0001203\n",
      "[259,   250] loss: 0.0001349\n",
      "[260,   250] loss: 0.0001087\n",
      "[261,   250] loss: 0.0001500\n",
      "[262,   250] loss: 0.0000609\n",
      "[263,   250] loss: 0.0001003\n",
      "[264,   250] loss: 0.0000869\n",
      "[265,   250] loss: 0.0001151\n",
      "[266,   250] loss: 0.0001042\n",
      "[267,   250] loss: 0.0000669\n",
      "[268,   250] loss: 0.0001032\n",
      "[269,   250] loss: 0.0001275\n",
      "[270,   250] loss: 0.0000618\n",
      "[271,   250] loss: 0.0000894\n",
      "[272,   250] loss: 0.0001600\n",
      "[273,   250] loss: 0.0000547\n",
      "[274,   250] loss: 0.0000819\n",
      "[275,   250] loss: 0.0000634\n",
      "[276,   250] loss: 0.0000794\n",
      "[277,   250] loss: 0.0001009\n",
      "[278,   250] loss: 0.0000750\n",
      "[279,   250] loss: 0.0001017\n",
      "[280,   250] loss: 0.0001114\n",
      "[281,   250] loss: 0.0000878\n",
      "[282,   250] loss: 0.0000773\n",
      "[283,   250] loss: 0.0001564\n",
      "[284,   250] loss: 0.0001093\n",
      "[285,   250] loss: 0.0001017\n",
      "[286,   250] loss: 0.0000889\n",
      "[287,   250] loss: 0.0001156\n",
      "[288,   250] loss: 0.0001856\n",
      "[289,   250] loss: 0.0002110\n",
      "[290,   250] loss: 0.0000554\n",
      "[291,   250] loss: 0.0001189\n",
      "[292,   250] loss: 0.0001234\n",
      "[293,   250] loss: 0.0000883\n",
      "[294,   250] loss: 0.0000591\n",
      "[295,   250] loss: 0.0001348\n",
      "[296,   250] loss: 0.0001267\n",
      "[297,   250] loss: 0.0000549\n",
      "[298,   250] loss: 0.0000882\n",
      "[299,   250] loss: 0.0001172\n",
      "[300,   250] loss: 0.0000610\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):  \n",
    "    \n",
    "    if epoch == 150:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    elif epoch == 250:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        show_period = 250\n",
    "        \n",
    "        if i % show_period == show_period-1:    \n",
    "            print('[%d, %5d] loss: %.7f' %\n",
    "                  (epoch + 1, i + 1, running_loss / show_period))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 93 %\n",
      "Accuracy of   car : 95 %\n",
      "Accuracy of  bird : 91 %\n",
      "Accuracy of   cat : 87 %\n",
      "Accuracy of  deer : 94 %\n",
      "Accuracy of   dog : 89 %\n",
      "Accuracy of  frog : 95 %\n",
      "Accuracy of horse : 96 %\n",
      "Accuracy of  ship : 94 %\n",
      "Accuracy of truck : 95 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "                \n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
